# -*- coding: utf-8 -*-
#+TITLE: Faithful and Efficient Simulation of High Performance Linpack
#+AUTHOR: Tom Cornebize, Arnaud Legrand, Christian Heinrich
#+LANGUAGE: EN
#+STARTUP: overview indent inlineimages logdrawer hidestars

* Table of content                                                      :TOC:
- [[#abstract][Abstract]]
- [[#introduction][Introduction]]
  - [[#overview][Overview]]
  - [[#dependencies][Dependencies]]
  - [[#peanut-the-experiment-engine][Peanut, the experiment engine]]
- [[#platform-calibration][Platform calibration]]
  - [[#mpi-calibration][MPI calibration]]
  - [[#kernel-calibration][Kernel calibration]]
  - [[#dgemm-calibration][DGEMM calibration]]
- [[#simulation][Simulation]]
  - [[#complete-execution][Complete execution]]
  - [[#partial-execution-with-tracing][Partial execution with tracing]]
- [[#real-execution][Real execution]]
  - [[#complete-execution-1][Complete execution]]
  - [[#partial-execution-with-tracing-1][Partial execution with tracing]]

* Abstract
With a power consumption of several MW per hour on a TOP500 machine,
running applications on supercomputers at scale solely to optimize
their performance is extremely expensive. Likewise, High-Performance Linpack (HPL),
the benchmark used to rank supercomputers in the TOP500, requires a
careful tuning of many parameters (problem size, grid arrangement,
granularity, collective operation algorithms, etc.) and supports
exploration of the most common and fundamental performance issues and
their solutions. In this article, we explain how we both extended the
SimGrid's SMPI simulator and slightly modified the open-source version
of HPL to allow a fast emulation on a single commodity server at the
scale of a supercomputer. We explain how to model the different
components (network, BLAS, ...) and show that a careful modeling of
both spatial and temporal node variability allows us to obtain faithful
predictions within a few percents of real experiments.
* Introduction
** Overview
This document contains links to experimental results as well as a guideline on
how to reproduce them. We used the [[https://www.grid5000.fr/][Grid5000]] testbed for all our experiments.

The structure of this repository is the following:
- In [[file:exp_input][exp_input]] are located the inputs of each experiment (=.csv= files with
  eventual =.xml= and =.yaml= files)
- In [[file:exp_output][exp_output]] are located the outputs of each experiment (=.zip= files)
- In [[file:exp_analysis][exp_analysis]] are located the analysis that we made for each experiment
  (=.ipynb files=)
- The directory [[file:repositories][repositories]] contains submodules. They are pointers to the
  different resources used by our scripts. They are not strictly needed here, as
  the dependencies are installed with hard-links and a (small but relevant)
  subset of the data has been copied in the repository.
** Dependencies
One need to install a few Python packages to run the experiments and/or the
analysis scripts. They can be installed with the following commands (make sure
to replace =pip= by =pip3= if the default Python on your system is Python 2).
#+begin_src sh :results output :exports both
# Common libraries
pip install --user pyyaml lxml statsmodels ipython jupyterlab pandas plotnine

# Pytree (library to compute model trees)
pip install --user https://github.com/Ezibenroc/pytree/releases/download/0.0.6/pytree-0.0.6-py3-none-any.whl

# Peanut (experiment engine, see below)
pip install --user https://github.com/Ezibenroc/peanut/releases/download/0.0.0/peanut-0.0.0-py3-none-any.whl
#+end_src
** Peanut, the experiment engine
We extensively used [[https://github.com/Ezibenroc/peanut/][peanut]], a tool to automatically deploy and run experiments
on Grid5000. The result produced by this tool is a =.zip= archive containing at
least these files:
- =info.yaml= contains various information about the job such as:
  + ID of the job
  + command used to run this experiment
  + OS image used for the deployment
  + git hash of =peanut= at the time
  + list of the nodes used for the experiment
  + git repositories cloned for this experiment
  + versions of the kernel, GCC and MPI
- =oarstat.yaml= contains information about the job reservation itself (e.g.,
  start and stop times)
- =commands.log= contains a human-readable dump of all the commands executed for
  the experiment, both on the frontend node and the compute nodes
- =history.json= contains a dump of all the commands executed, with their return
  code, output (both stdout and stderr), start and stop times
- =information= is a directory containing one subdirectory per node, each one
  holding relevant information about the node like a copy of =/proc/cpuinfo= or
  the output of the command =env=

In addition, the archives will usally contain files specific to a given
experiment. These files may be:
- install files, describing some configurations for the installation of the
  nodes (e.g., should we run a complete execution of HPL, or only the first
  iterations)
- experiment files, describing relevant parameters (e.g., the matrix rank and
  the broadcast algorithm for HPL)
- result files, describing the result of the experiment (e.g., the duration and
  performance of the HPL runs)
* Platform calibration
To have a faithful model of an existing platform, it is essential to perform
several calibrations.
** MPI calibration
#+begin_src sh :results output :exports both
python -m peanut MPICalibration run tocornebize --cluster dahu --deploy debian9-x64-base \
       --walltime 00:30:00 --nbnodes 2 --expfile exp_input/mpi_calibration/exp.csv --batch
#+end_src
** Kernel calibration
#+begin_src sh :results output :exports both
python -m peanut HPL run tocornebize --deploy debian9-x64-base --cluster dahu --nbnodes 8 \
       --walltime 01:00:00 --expfile exp_input/kernel_calibration/exp.csv \
       --installfile exp_input/kernel_calibration/install.yaml --batch
#+end_src
** DGEMM calibration
#+begin_src sh :results output :exports both
for i in {1..32} ; do
    python -m peanut BLASCalibration run tocornebize --deploy debian9-x64-base --nodes dahu-$i \
           --nbnodes 1 --walltime 03:30:00 --expfile exp_input/dgemm_calibration/exp.csv --batch
    sleep 3  # to be nice with the job scheduler of the cluster
done
#+end_src
* Simulation
** Complete execution
Note: here, we launch several independent jobs to run different simulations in parallel.
#+begin_src sh :results output :exports both
for mode in exp_input/simulation/complete/{heterogeneous,homogeneous}; do
    for exp in exp_input/simulation/complete/exp_*.csv; do
        for ins in ${mode}/install_*.yaml; do
            python -m peanut SMPIHPL run tocornebize --deploy debian9-x64-base --cluster dahu \
                   --nbnodes 1 --walltime 14:00:00 --expfile ${exp} exp_input/simulation/dahu.xml \
                   --installfile ${ins} --batch
            sleep 3  # to be nice with the job scheduler of the cluster
        done
    done
done
#+end_src
** Partial execution with tracing
#+begin_src sh :results output :exports both
python -m peanut SMPIHPL run tocornebize --deploy debian9-x64-base --cluster dahu --nbnodes 1 \
       --walltime 01:00:00 --expfile exp_input/simulation/{dahu.xml,tracing/exp.csv} \
       --installfile exp_input/simulation/tracing/install.yaml --batch
#+end_src
* Real execution
** Complete execution
#+begin_src sh :results output :exports both
python -m peanut HPL run tocornebize --deploy debian9-x64-base --cluster dahu --nbnodes 32 \
       --walltime 13:30:00 --expfile exp_input/real_execution/complete/exp.csv \
       --installfile exp_input/real_execution/complete/install.yaml --batch
#+end_src
** Partial execution with tracing
Very similar to the kernel calibration, this time we control exactly which nodes
we want.
#+begin_src sh :results output :exports both
python -m peanut HPL run tocornebize --deploy debian9-x64-base --walltime 01:00:00 \
       --nodes dahu-1 dahu-2 dahu-3 dahu-4 dahu-5 dahu-6 dahu-7 dahu-8 --nbnodes 8  \
       --installfile exp_input/real_execution/tracing/install.yaml --batch \
       --expfile exp_input/real_execution/tracing/exp.csv
#+end_src
